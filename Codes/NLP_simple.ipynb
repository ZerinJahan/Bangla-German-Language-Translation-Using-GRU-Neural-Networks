{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_simple.ipynb","provenance":[],"mount_file_id":"1A0CIfSsSpyOZR5pSTHt7IRbMiCaIELSS","authorship_tag":"ABX9TyNjXcJAnWKcWOs8C87Zj4M0"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"mUrZlxG0_kBh","executionInfo":{"status":"ok","timestamp":1603633558204,"user_tz":-360,"elapsed":3653,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}}},"source":["import collections\n","import numpy as np\n","import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Model\n","from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n","from keras.layers.embeddings import Embedding\n","from keras.optimizers import Adam\n","from keras.losses import sparse_categorical_crossentropy"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"De1-ZiJA_l9K","executionInfo":{"status":"ok","timestamp":1603633568478,"user_tz":-360,"elapsed":1091,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}}},"source":["import os\n","\n","def load_data(path):\n","    \"\"\"\n","    Load dataset\n","    \"\"\"\n","    input_file = os.path.join(path)\n","    with open(input_file, \"r\", encoding ='utf=8') as f:\n","        data = f.read()\n","\n","    return data.split('\\n')\n","\n","\n","from keras.losses import sparse_categorical_crossentropy\n","from keras.models import Sequential\n","from keras.preprocessing.text import Tokenizer\n","from keras.utils import to_categorical\n","\n","\n","def _test_model(model, input_shape, output_sequence_length, french_vocab_size):\n","    if isinstance(model, Sequential):\n","        model = model.model\n","\n","    assert model.input_shape == (None, *input_shape[1:]),'Wrong input shape. Found input shape {} using parameter input_shape={}'.format(model.input_shape, input_shape)\n","\n","    assert model.output_shape == (None, output_sequence_length, french_vocab_size),'Wrong output shape. Found output shape {} using parameters output_sequence_length={} and french_vocab_size={}'.format(model.output_shape, output_sequence_length, french_vocab_size)\n","\n","    assert len(model.loss_functions) > 0,'No loss function set.  Apply the `compile` function to the model.'\n","\n","    assert sparse_categorical_crossentropy in model.loss_functions,'Not using `sparse_categorical_crossentropy` function for loss.'\n","\n","\n","def test_tokenize(tokenize):\n","    sentences = [\n","        'আমি এই উপন্যাস আগেও পড়েছি।',\n","        'টম খুব খোলামেলা মানুষ।',\n","        'তুমি কি কখনো হেলিকপ্টারে বসেছো?']\n","    tokenized_sentences, tokenizer = tokenize(sentences)\n","    assert tokenized_sentences == tokenizer.texts_to_sequences(sentences),\\\n","        'Tokenizer returned and doesn\\'t generate the same sentences as the tokenized sentences returned. '\n","\n","\n","def test_pad(pad):\n","    tokens = [\n","        [i for i in range(4)],\n","        [i for i in range(6)],\n","        [i for i in range(3)]]\n","    padded_tokens = pad(tokens)\n","    padding_id = padded_tokens[0][-1]\n","    true_padded_tokens = np.array([\n","        [i for i in range(4)] + [padding_id]*2,\n","        [i for i in range(6)],\n","        [i for i in range(3)] + [padding_id]*3])\n","    assert isinstance(padded_tokens, np.ndarray),\\\n","        'Pad returned the wrong type.  Found {} type, expected numpy array type.'\n","    assert np.all(padded_tokens == true_padded_tokens), 'Pad returned the wrong results.'\n","\n","    padded_tokens_using_length = pad(tokens, 9)\n","    assert np.all(padded_tokens_using_length == np.concatenate((true_padded_tokens, np.full((3, 3), padding_id)), axis=1)),\\\n","        'Using length argument return incorrect results'\n","\n","\n","def test_simple_model(simple_model):\n","    input_shape = (80000, 17, 1)\n","    output_sequence_length = 17\n","    english_vocab_size = 12201\n","    french_vocab_size = 14157\n","\n","    model = simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n","    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n","\n","\n","def test_embed_model(embed_model):\n","    input_shape = (200000, 17)\n","    output_sequence_length = 17\n","    english_vocab_size = 12201\n","    french_vocab_size = 14157\n","\n","    model = embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n","    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n","\n","\n","def test_encdec_model(encdec_model):\n","    input_shape = (200000, 17, 1)\n","    output_sequence_length = 17\n","    english_vocab_size = 12201\n","    french_vocab_size = 14157\n","\n","    model = encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n","    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n","\n","\n","def test_bd_model(bd_model):\n","    input_shape = (200000, 17, 1)\n","    output_sequence_length = 17\n","    english_vocab_size = 12201\n","    french_vocab_size = 14157\n","\n","    model = bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n","    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n","\n","\n","def test_model_final(model_final):\n","    input_shape = (200000, 17)\n","    output_sequence_length = 17\n","    english_vocab_size = 12201\n","    french_vocab_size = 14157\n","\n","    model = model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n","    _test_model(model, input_shape, output_sequence_length, french_vocab_size)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"sJ8q5odW_maI","executionInfo":{"status":"ok","timestamp":1603633600877,"user_tz":-360,"elapsed":3559,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"7d745029-2bac-46fc-862c-249a0625c5a8","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["english_sentences = load_data('/content/drive/My Drive/NLP_csv/80kban.txt')\n","french_sentences = load_data('/content/drive/My Drive/NLP_csv/80kger.txt')\n","print('Dataset Loaded')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Dataset Loaded\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"I8qKzBsY_mhv","executionInfo":{"status":"ok","timestamp":1603633600880,"user_tz":-360,"elapsed":932,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"1ba87084-0c00-4cda-9b6d-30f1f2e13adc","colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["for sample_i in range(2):\n","    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n","    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["small_vocab_en Line 1:  যান.\n","small_vocab_fr Line 1:  Geh.\n","small_vocab_en Line 2:  নমস্কার!\n","small_vocab_fr Line 2:  Hallo!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"z-B1Yh-o_mqB","executionInfo":{"status":"ok","timestamp":1603633604200,"user_tz":-360,"elapsed":1004,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"4e2650db-7c16-4f3a-9e7d-357bf63f7d9a","colab":{"base_uri":"https://localhost:8080/","height":176}},"source":["english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n","french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n","print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n","print('{} unique English words.'.format(len(english_words_counter)))\n","print('10 Most common words in the English dataset:')\n","print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n","print()\n","print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n","print('{} unique French words.'.format(len(french_words_counter)))\n","print('10 Most common words in the French dataset:')\n","print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["335708 English words.\n","14101 unique English words.\n","10 Most common words in the English dataset:\n","\"আমি\" \"টম\" \"এটা\" \"কি\" \"তুমি\" \"আমার\" \"না।\" \"তোমার\" \"সে\" \"একটা\"\n","\n","356660 French words.\n","23231 unique French words.\n","10 Most common words in the French dataset:\n","\"Tom\" \"Ich\" \"ist\" \"Sie\" \"nicht\" \"das\" \"du\" \"Das\" \"hat\" \"Er\"\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Jk7Qxk69_myq","executionInfo":{"status":"ok","timestamp":1603633608189,"user_tz":-360,"elapsed":965,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"1627e7a5-cb7b-4bf8-a1de-9ea4e91d8f26","colab":{"base_uri":"https://localhost:8080/","height":232}},"source":["def tokenize(x):\n","    x_tk = Tokenizer(char_level = False)\n","    x_tk.fit_on_texts(x)\n","    return x_tk.texts_to_sequences(x), x_tk\n","text_sentences = [\n","    'আমি এই উপন্যাস আগেও পড়েছি।',\n","    'টম খুব খোলামেলা মানুষ।',\n","    'তুমি কি কখনো হেলিকপ্টারে বসেছো?']\n","text_tokenized, text_tokenizer = tokenize(text_sentences)\n","print(text_tokenizer.word_index)\n","print()\n","for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n","    print('Sequence {} in x'.format(sample_i + 1))\n","    print('  Input:  {}'.format(sent))\n","    print('  Output: {}'.format(token_sent))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["{'আমি': 1, 'এই': 2, 'উপন্যাস': 3, 'আগেও': 4, 'পড়েছি।': 5, 'টম': 6, 'খুব': 7, 'খোলামেলা': 8, 'মানুষ।': 9, 'তুমি': 10, 'কি': 11, 'কখনো': 12, 'হেলিকপ্টারে': 13, 'বসেছো': 14}\n","\n","Sequence 1 in x\n","  Input:  আমি এই উপন্যাস আগেও পড়েছি।\n","  Output: [1, 2, 3, 4, 5]\n","Sequence 2 in x\n","  Input:  টম খুব খোলামেলা মানুষ।\n","  Output: [6, 7, 8, 9]\n","Sequence 3 in x\n","  Input:  তুমি কি কখনো হেলিকপ্টারে বসেছো?\n","  Output: [10, 11, 12, 13, 14]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rtd3AxgJ_m7Y","executionInfo":{"status":"ok","timestamp":1603633612795,"user_tz":-360,"elapsed":993,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"859ebfad-a5eb-46d4-afeb-c5f22e20a586","colab":{"base_uri":"https://localhost:8080/","height":176}},"source":["# import project_tests as tests\n","def pad(x, length=None):\n","    if length is None:\n","        length = max([len(sentence) for sentence in x])\n","    return pad_sequences(x, maxlen = length, padding = 'post')\n","test_pad(pad)\n","# Pad Tokenized output\n","test_pad = pad(text_tokenized)\n","for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n","    print('Sequence {} in x'.format(sample_i + 1))\n","    print('  Input:  {}'.format(np.array(token_sent)))\n","    print('  Output: {}'.format(pad_sent))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Sequence 1 in x\n","  Input:  [1 2 3 4 5]\n","  Output: [1 2 3 4 5]\n","Sequence 2 in x\n","  Input:  [6 7 8 9]\n","  Output: [6 7 8 9 0]\n","Sequence 3 in x\n","  Input:  [10 11 12 13 14]\n","  Output: [10 11 12 13 14]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RD6Tvapu_nD6","executionInfo":{"status":"ok","timestamp":1603633620975,"user_tz":-360,"elapsed":4744,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"3119bfec-8bd5-45d7-e38a-8ad8d1ace420","colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["def preprocess(x, y):\n","    preprocess_x, x_tk = tokenize(x)\n","    preprocess_y, y_tk = tokenize(y)\n","    preprocess_x = pad(preprocess_x)\n","    preprocess_y = pad(preprocess_y)\n","    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n","    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n","    return preprocess_x, preprocess_y, x_tk, y_tk\n","\n","preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer = preprocess(english_sentences, french_sentences)    \n","max_english_sequence_length = preproc_english_sentences.shape[1]\n","max_french_sequence_length = preproc_french_sentences.shape[1]\n","english_vocab_size = len(english_tokenizer.word_index)\n","french_vocab_size = len(french_tokenizer.word_index)\n","print('Data Preprocessed')\n","print(\"Max English sentence length:\", max_english_sequence_length)\n","print(\"Max French sentence length:\", max_french_sequence_length)\n","print(\"English vocabulary size:\", english_vocab_size)\n","print(\"French vocabulary size:\", french_vocab_size)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Data Preprocessed\n","Max English sentence length: 17\n","Max French sentence length: 17\n","English vocabulary size: 12201\n","French vocabulary size: 14157\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SfK6276s_nM5","executionInfo":{"status":"ok","timestamp":1603633622253,"user_tz":-360,"elapsed":1273,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"5e5ff757-0e2e-4f39-883e-5c7e8cc87184","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["def logits_to_text(logits, tokenizer):\n","    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n","    index_to_words[0] = '<PAD>'\n","    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n","print('`logits_to_text` function loaded.')"],"execution_count":11,"outputs":[{"output_type":"stream","text":["`logits_to_text` function loaded.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ug1Z-hC9_nV5","executionInfo":{"status":"ok","timestamp":1603658033744,"user_tz":-360,"elapsed":24404680,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"686ac8fd-6c43-4ebb-9669-a89f6725971c","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n","    learning_rate = 1e-2\n","    input_seq = Input(input_shape[1:])\n","    rnn = GRU(128, return_sequences = True)(input_seq)\n","    logits = TimeDistributed(Dense(french_vocab_size))(rnn)\n","    model = Model(input_seq, Activation('softmax')(logits))\n","    model.compile(loss=sparse_categorical_crossentropy,\n","                  optimizer=Adam(learning_rate),\n","                  metrics=['accuracy'])\n","    \n","    return model\n","# test_simple_model(simple_model)\n","tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n","tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n","# Train the neural network\n","simple_rnn_model = simple_model(\n","    tmp_x.shape,\n","    max_french_sequence_length,\n","    english_vocab_size+1,\n","    french_vocab_size+1)\n","simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=100, epochs=30, validation_split=0.2)\n","# Print prediction(s)\n","print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Epoch 1/30\n","640/640 [==============================] - 807s 1s/step - loss: 1.6774 - accuracy: 0.7751 - val_loss: 1.9407 - val_accuracy: 0.7306\n","Epoch 2/30\n","640/640 [==============================] - 805s 1s/step - loss: 1.4183 - accuracy: 0.7916 - val_loss: 1.9046 - val_accuracy: 0.7358\n","Epoch 3/30\n","640/640 [==============================] - 807s 1s/step - loss: 1.3496 - accuracy: 0.7952 - val_loss: 1.8801 - val_accuracy: 0.7393\n","Epoch 4/30\n","640/640 [==============================] - 804s 1s/step - loss: 1.3089 - accuracy: 0.7969 - val_loss: 1.8748 - val_accuracy: 0.7405\n","Epoch 5/30\n","640/640 [==============================] - 817s 1s/step - loss: 1.2823 - accuracy: 0.7979 - val_loss: 1.8697 - val_accuracy: 0.7409\n","Epoch 6/30\n","640/640 [==============================] - 811s 1s/step - loss: 1.2581 - accuracy: 0.7988 - val_loss: 1.8626 - val_accuracy: 0.7406\n","Epoch 7/30\n","640/640 [==============================] - 809s 1s/step - loss: 1.2420 - accuracy: 0.7994 - val_loss: 1.8716 - val_accuracy: 0.7422\n","Epoch 8/30\n","640/640 [==============================] - 809s 1s/step - loss: 1.2271 - accuracy: 0.8001 - val_loss: 1.8865 - val_accuracy: 0.7412\n","Epoch 9/30\n","640/640 [==============================] - 817s 1s/step - loss: 1.2143 - accuracy: 0.8003 - val_loss: 1.8847 - val_accuracy: 0.7424\n","Epoch 10/30\n","640/640 [==============================] - 816s 1s/step - loss: 1.2048 - accuracy: 0.8005 - val_loss: 1.8844 - val_accuracy: 0.7427\n","Epoch 11/30\n","640/640 [==============================] - 815s 1s/step - loss: 1.1946 - accuracy: 0.8008 - val_loss: 1.8848 - val_accuracy: 0.7422\n","Epoch 12/30\n","640/640 [==============================] - 820s 1s/step - loss: 1.1867 - accuracy: 0.8009 - val_loss: 1.8829 - val_accuracy: 0.7426\n","Epoch 13/30\n","640/640 [==============================] - 812s 1s/step - loss: 1.1779 - accuracy: 0.8013 - val_loss: 1.8962 - val_accuracy: 0.7428\n","Epoch 14/30\n","640/640 [==============================] - 814s 1s/step - loss: 1.1724 - accuracy: 0.8012 - val_loss: 1.8976 - val_accuracy: 0.7431\n","Epoch 15/30\n","640/640 [==============================] - 811s 1s/step - loss: 1.1664 - accuracy: 0.8016 - val_loss: 1.9098 - val_accuracy: 0.7425\n","Epoch 16/30\n","640/640 [==============================] - 810s 1s/step - loss: 1.1623 - accuracy: 0.8017 - val_loss: 1.9082 - val_accuracy: 0.7424\n","Epoch 17/30\n","640/640 [==============================] - 811s 1s/step - loss: 1.1610 - accuracy: 0.8017 - val_loss: 1.9113 - val_accuracy: 0.7433\n","Epoch 18/30\n","640/640 [==============================] - 818s 1s/step - loss: 1.1523 - accuracy: 0.8021 - val_loss: 1.9102 - val_accuracy: 0.7428\n","Epoch 19/30\n","640/640 [==============================] - 822s 1s/step - loss: 1.1480 - accuracy: 0.8023 - val_loss: 1.9179 - val_accuracy: 0.7436\n","Epoch 20/30\n","640/640 [==============================] - 817s 1s/step - loss: 1.1431 - accuracy: 0.8024 - val_loss: 1.9243 - val_accuracy: 0.7424\n","Epoch 21/30\n","640/640 [==============================] - 811s 1s/step - loss: 1.1410 - accuracy: 0.8026 - val_loss: 1.9232 - val_accuracy: 0.7439\n","Epoch 22/30\n","640/640 [==============================] - 818s 1s/step - loss: 1.1374 - accuracy: 0.8027 - val_loss: 1.9165 - val_accuracy: 0.7432\n","Epoch 23/30\n","640/640 [==============================] - 816s 1s/step - loss: 1.1354 - accuracy: 0.8028 - val_loss: 1.9227 - val_accuracy: 0.7439\n","Epoch 24/30\n","640/640 [==============================] - 806s 1s/step - loss: 1.1298 - accuracy: 0.8032 - val_loss: 1.9240 - val_accuracy: 0.7432\n","Epoch 25/30\n","640/640 [==============================] - 811s 1s/step - loss: 1.1290 - accuracy: 0.8031 - val_loss: 1.9334 - val_accuracy: 0.7437\n","Epoch 26/30\n","640/640 [==============================] - 807s 1s/step - loss: 1.1248 - accuracy: 0.8031 - val_loss: 1.9336 - val_accuracy: 0.7433\n","Epoch 27/30\n","640/640 [==============================] - 813s 1s/step - loss: 1.1217 - accuracy: 0.8035 - val_loss: 1.9357 - val_accuracy: 0.7431\n","Epoch 28/30\n","640/640 [==============================] - 814s 1s/step - loss: 1.1209 - accuracy: 0.8033 - val_loss: 1.9447 - val_accuracy: 0.7437\n","Epoch 29/30\n","640/640 [==============================] - 810s 1s/step - loss: 1.1195 - accuracy: 0.8033 - val_loss: 1.9384 - val_accuracy: 0.7441\n","Epoch 30/30\n","640/640 [==============================] - 809s 1s/step - loss: 1.1166 - accuracy: 0.8036 - val_loss: 1.9398 - val_accuracy: 0.7419\n","ist ist <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NhBHJ1UG_nev","executionInfo":{"status":"ok","timestamp":1603658120977,"user_tz":-360,"elapsed":10133,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"d4aa4ab8-548f-495c-d919-1863f93ad14e","colab":{"base_uri":"https://localhost:8080/","height":123}},"source":["def final_predictions(x, y, x_tk, y_tk):\n","    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n","    y_id_to_word[0] = '<PAD>'\n","    sentence = ''\n","    sentence = [x_tk.word_index[word] for word in sentence.split()]\n","    debug1 = sentence\n","    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n","    debug2 = sentence\n","    sentences = np.array([sentence[0], x[0]])\n","    debug3 = sentences\n","    predictions = simple_rnn_model.predict(sentences, len(sentences))\n","    debug4 = predictions\n","    print('Sample 1:')\n","    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n","    print('ওহে')\n","    print('Sample 2:')\n","    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n","    print(' '.join([y_id_to_word[np.max(x)] for x in y[1209]]))\n","    a = []\n","    for i in range(0,len(french_sentences)):\n","        debug5 = \" \".join([y_id_to_word[np.max(x)] for x in y[i]])\n","        if i  == 1:\n","            debug4 = debug5\n","        a.append(debug5)        \n","    from pandas import DataFrame\n","    df = DataFrame(a,columns=[\"predicted string\"])\n","    df[\"predicted string\"]= df[\"predicted string\"].str.replace(\"<PAD>\", \"\", case = False) \n","    df[\"actual language\"] = french_sentences\n","\n","    \n","    return debug1, debug2, debug3, debug4, debug5, a, df\n","debug1, debug2, debug3, debug4, debug5,  a, df = final_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)\n","#df.to_csv(\"jekhane khushi save kore ne sagol.csv\")"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Sample 1:\n","ich bin’s <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","ওহে\n","Sample 2:\n","ist ist <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","prüfen sie das nach <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Cb4Ntlqb_nnn","executionInfo":{"status":"ok","timestamp":1603658124604,"user_tz":-360,"elapsed":1048,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}}},"source":["df_test = df"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"y2L6X_FNANbm","executionInfo":{"status":"ok","timestamp":1603658127547,"user_tz":-360,"elapsed":1036,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}}},"source":["remove_characters = [\"?\", \".\",\"!\",\",\"]\n","\n","for c in remove_characters:\n","    df_test[\"actual language\"] =  df_test[\"actual language\"].str.replace(c,\"\")\n","\n","df_test[\"actual language\"] = df_test[\"actual language\"] .str.lower()"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xu4S6rF5ANrJ","executionInfo":{"status":"ok","timestamp":1603658130263,"user_tz":-360,"elapsed":1131,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"78af47f0-1ca4-41bd-d135-614491e2c991","colab":{"base_uri":"https://localhost:8080/","height":406}},"source":["df_test"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>predicted string</th>\n","      <th>actual language</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>geh</td>\n","      <td>geh</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>hallo</td>\n","      <td>hallo</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>grüß gott</td>\n","      <td>grüß gott</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>lauf</td>\n","      <td>lauf</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>lauf</td>\n","      <td>lauf</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>79995</th>\n","      <td>ich habe nicht so viel mut wie ihr</td>\n","      <td>ich habe nicht so viel mut wie ihr</td>\n","    </tr>\n","    <tr>\n","      <th>79996</th>\n","      <td>das habe ich nicht vor</td>\n","      <td>das habe ich nicht vor</td>\n","    </tr>\n","    <tr>\n","      <th>79997</th>\n","      <td>ich weiss toms nummer nicht</td>\n","      <td>ich weiss toms nummer nicht</td>\n","    </tr>\n","    <tr>\n","      <th>79998</th>\n","      <td>ich weiß nicht wie ich das gemacht habe</td>\n","      <td>ich weiß nicht wie ich das gemacht habe</td>\n","    </tr>\n","    <tr>\n","      <th>79999</th>\n","      <td>ich weiß nicht wie ich es machen soll</td>\n","      <td>ich weiß nicht wie ich es machen soll</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>80000 rows × 2 columns</p>\n","</div>"],"text/plain":["                                       predicted string                          actual language\n","0                                   geh                                                      geh\n","1                                 hallo                                                    hallo\n","2                              grüß gott                                               grüß gott\n","3                                  lauf                                                     lauf\n","4                                  lauf                                                     lauf\n","...                                                 ...                                      ...\n","79995       ich habe nicht so viel mut wie ihr                ich habe nicht so viel mut wie ihr\n","79996                das habe ich nicht vor                               das habe ich nicht vor\n","79997           ich weiss toms nummer nicht                          ich weiss toms nummer nicht\n","79998  ich weiß nicht wie ich das gemacht habe           ich weiß nicht wie ich das gemacht habe\n","79999    ich weiß nicht wie ich es machen soll             ich weiß nicht wie ich es machen soll\n","\n","[80000 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"lMkYXXWtAWG1","executionInfo":{"status":"ok","timestamp":1603658946157,"user_tz":-360,"elapsed":812504,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"9b41190c-0508-4bcc-d5d7-b12d14b1cb0c","colab":{"base_uri":"https://localhost:8080/","height":158}},"source":["col_1 = df['predicted string'].tolist()\n","col_2 = df[\"actual language\"].tolist()\n","\n","\n","from nltk.translate.bleu_score import corpus_bleu\n","#reference = [['this', 'is', 'a', 'test'], ['this', 'is' 'test']]\n","#candidate = ['this', 'is', 'a', 'test']\n","score1 = corpus_bleu(col_2, col_1, weights=(1, 0, 0, 0))\n","score2 = corpus_bleu(col_2, col_1, weights=(0.5, 0.5, 0, 0))\n","score3 = corpus_bleu(col_2, col_1, weights=(0.33, 0.33, 0.33, 0))\n","score4 = corpus_bleu(col_2, col_1, weights=(0.25, 0.25, 0.25, 0.25))\n","score21 = corpus_bleu(col_2, col_1, weights=(0.5, 0.5, 0, 0))\n","score31 = corpus_bleu(col_2, col_1, weights=(0.33, 0.33, 0.33, 0))\n","score41 = corpus_bleu(col_2, col_1, weights=(0.25, 0.25, 0.25, 0.25))\n","print(score1)\n","print(score2)\n","print(score3)\n","print(score4)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"],"name":"stderr"},{"output_type":"stream","text":["0.3561822299031535\n","0.5968100450756116\n","0.7112984167321372\n","0.7725348180345087\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VkaF1N1cATNE","executionInfo":{"status":"ok","timestamp":1603659418916,"user_tz":-360,"elapsed":467026,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"bc58f304-3645-4561-fa0b-a2aa1dcbc4b3","colab":{"base_uri":"https://localhost:8080/","height":158}},"source":["score11 = corpus_bleu(col_2, col_1, weights=(1, 0, 0, 0))\n","score21 = corpus_bleu(col_2, col_1, weights=(0, 1, 0, 0))\n","score31 = corpus_bleu(col_2, col_1, weights=(0, 0, 1, 0))\n","score41 = corpus_bleu(col_2, col_1, weights=(0, 0, 0, 1))\n","print(score11)\n","print(score21)\n","print(score31)\n","print(score41)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"],"name":"stderr"},{"output_type":"stream","text":["0.3561822299031535\n","1.0\n","1.0\n","1.0\n"],"name":"stdout"}]}]}