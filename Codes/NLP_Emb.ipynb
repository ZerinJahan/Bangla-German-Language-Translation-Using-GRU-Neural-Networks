{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_Emb.ipynb","provenance":[],"mount_file_id":"1_hf9DvHeP0FLOWBkBewjohf5vPIrIw8O","authorship_tag":"ABX9TyMqn1hmmChHeKiHYD7rDnqA"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"BomXwTwDA-NU","executionInfo":{"status":"ok","timestamp":1603633931763,"user_tz":-360,"elapsed":2858,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}}},"source":["import collections\n","import numpy as np\n","import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Model\n","from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n","from keras.layers.embeddings import Embedding\n","from keras.optimizers import Adam\n","from keras.losses import sparse_categorical_crossentropy"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"XWX34DH1A_Hm","executionInfo":{"status":"ok","timestamp":1603633940574,"user_tz":-360,"elapsed":975,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}}},"source":["import os\n","\n","def load_data(path):\n","    \"\"\"\n","    Load dataset\n","    \"\"\"\n","    input_file = os.path.join(path)\n","    with open(input_file, \"r\", encoding ='utf=8') as f:\n","        data = f.read()\n","\n","    return data.split('\\n')\n","\n","\n","from keras.losses import sparse_categorical_crossentropy\n","from keras.models import Sequential\n","from keras.preprocessing.text import Tokenizer\n","from keras.utils import to_categorical\n","\n","\n","def _test_model(model, input_shape, output_sequence_length, french_vocab_size):\n","    if isinstance(model, Sequential):\n","        model = model.model\n","\n","    assert model.input_shape == (None, *input_shape[1:]),'Wrong input shape. Found input shape {} using parameter input_shape={}'.format(model.input_shape, input_shape)\n","\n","    assert model.output_shape == (None, output_sequence_length, french_vocab_size),'Wrong output shape. Found output shape {} using parameters output_sequence_length={} and french_vocab_size={}'.format(model.output_shape, output_sequence_length, french_vocab_size)\n","\n","    assert len(model.loss_functions) > 0,'No loss function set.  Apply the `compile` function to the model.'\n","\n","    assert sparse_categorical_crossentropy in model.loss_functions,'Not using `sparse_categorical_crossentropy` function for loss.'\n","\n","\n","def test_tokenize(tokenize):\n","    sentences = [\n","        'আমি এই উপন্যাস আগেও পড়েছি।',\n","        'টম খুব খোলামেলা মানুষ।',\n","        'তুমি কি কখনো হেলিকপ্টারে বসেছো?']\n","    tokenized_sentences, tokenizer = tokenize(sentences)\n","    assert tokenized_sentences == tokenizer.texts_to_sequences(sentences),\\\n","        'Tokenizer returned and doesn\\'t generate the same sentences as the tokenized sentences returned. '\n","\n","\n","def test_pad(pad):\n","    tokens = [\n","        [i for i in range(4)],\n","        [i for i in range(6)],\n","        [i for i in range(3)]]\n","    padded_tokens = pad(tokens)\n","    padding_id = padded_tokens[0][-1]\n","    true_padded_tokens = np.array([\n","        [i for i in range(4)] + [padding_id]*2,\n","        [i for i in range(6)],\n","        [i for i in range(3)] + [padding_id]*3])\n","    assert isinstance(padded_tokens, np.ndarray),\\\n","        'Pad returned the wrong type.  Found {} type, expected numpy array type.'\n","    assert np.all(padded_tokens == true_padded_tokens), 'Pad returned the wrong results.'\n","\n","    padded_tokens_using_length = pad(tokens, 9)\n","    assert np.all(padded_tokens_using_length == np.concatenate((true_padded_tokens, np.full((3, 3), padding_id)), axis=1)),\\\n","        'Using length argument return incorrect results'\n","\n","\n","def test_simple_model(simple_model):\n","    input_shape = (80000, 17, 1)\n","    output_sequence_length = 17\n","    english_vocab_size = 12201\n","    french_vocab_size = 14157\n","\n","    model = simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n","    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n","\n","\n","def test_embed_model(embed_model):\n","    input_shape = (200000, 17)\n","    output_sequence_length = 17\n","    english_vocab_size = 12201\n","    french_vocab_size = 14157\n","\n","    model = embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n","    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n","\n","\n","def test_encdec_model(encdec_model):\n","    input_shape = (200000, 17, 1)\n","    output_sequence_length = 17\n","    english_vocab_size = 12201\n","    french_vocab_size = 14157\n","\n","    model = encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n","    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n","\n","\n","def test_bd_model(bd_model):\n","    input_shape = (200000, 17, 1)\n","    output_sequence_length = 17\n","    english_vocab_size = 12201\n","    french_vocab_size = 14157\n","\n","    model = bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n","    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n","\n","\n","def test_model_final(model_final):\n","    input_shape = (200000, 17)\n","    output_sequence_length = 17\n","    english_vocab_size = 12201\n","    french_vocab_size = 14157\n","\n","    model = model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n","    _test_model(model, input_shape, output_sequence_length, french_vocab_size)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"OQQ3Q-0IA_PX","executionInfo":{"status":"ok","timestamp":1603633966626,"user_tz":-360,"elapsed":2748,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"0cc9c85d-0fd3-4a79-ce67-60cfd10883da","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["english_sentences = load_data('/content/drive/My Drive/NLP_csv/80kban.txt')\n","french_sentences = load_data('/content/drive/My Drive/NLP_csv/80kger.txt')\n","print('Dataset Loaded')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Dataset Loaded\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AGf4ERE9A_iI","executionInfo":{"status":"ok","timestamp":1603633967935,"user_tz":-360,"elapsed":922,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"4fdf0541-5f6b-40af-a434-47433d32fe23","colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["for sample_i in range(2):\n","    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n","    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["small_vocab_en Line 1:  যান.\n","small_vocab_fr Line 1:  Geh.\n","small_vocab_en Line 2:  নমস্কার!\n","small_vocab_fr Line 2:  Hallo!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TZfYHdJdA_pE","executionInfo":{"status":"ok","timestamp":1603633971387,"user_tz":-360,"elapsed":1173,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"f99b9064-c143-4411-a06c-661b9543fe45","colab":{"base_uri":"https://localhost:8080/","height":176}},"source":["english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n","french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n","print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n","print('{} unique English words.'.format(len(english_words_counter)))\n","print('10 Most common words in the English dataset:')\n","print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n","print()\n","print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n","print('{} unique French words.'.format(len(french_words_counter)))\n","print('10 Most common words in the French dataset:')\n","print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["335708 English words.\n","14101 unique English words.\n","10 Most common words in the English dataset:\n","\"আমি\" \"টম\" \"এটা\" \"কি\" \"তুমি\" \"আমার\" \"না।\" \"তোমার\" \"সে\" \"একটা\"\n","\n","356660 French words.\n","23231 unique French words.\n","10 Most common words in the French dataset:\n","\"Tom\" \"Ich\" \"ist\" \"Sie\" \"nicht\" \"das\" \"du\" \"Das\" \"hat\" \"Er\"\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OpsSSLmdA_vs","executionInfo":{"status":"ok","timestamp":1603633975039,"user_tz":-360,"elapsed":1143,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"88bc6dc9-def9-4059-e32a-27a119996f4a","colab":{"base_uri":"https://localhost:8080/","height":232}},"source":["def tokenize(x):\n","    x_tk = Tokenizer(char_level = False)\n","    x_tk.fit_on_texts(x)\n","    return x_tk.texts_to_sequences(x), x_tk\n","text_sentences = [\n","    'আমি এই উপন্যাস আগেও পড়েছি।',\n","    'টম খুব খোলামেলা মানুষ।',\n","    'তুমি কি কখনো হেলিকপ্টারে বসেছো?']\n","text_tokenized, text_tokenizer = tokenize(text_sentences)\n","print(text_tokenizer.word_index)\n","print()\n","for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n","    print('Sequence {} in x'.format(sample_i + 1))\n","    print('  Input:  {}'.format(sent))\n","    print('  Output: {}'.format(token_sent))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["{'আমি': 1, 'এই': 2, 'উপন্যাস': 3, 'আগেও': 4, 'পড়েছি।': 5, 'টম': 6, 'খুব': 7, 'খোলামেলা': 8, 'মানুষ।': 9, 'তুমি': 10, 'কি': 11, 'কখনো': 12, 'হেলিকপ্টারে': 13, 'বসেছো': 14}\n","\n","Sequence 1 in x\n","  Input:  আমি এই উপন্যাস আগেও পড়েছি।\n","  Output: [1, 2, 3, 4, 5]\n","Sequence 2 in x\n","  Input:  টম খুব খোলামেলা মানুষ।\n","  Output: [6, 7, 8, 9]\n","Sequence 3 in x\n","  Input:  তুমি কি কখনো হেলিকপ্টারে বসেছো?\n","  Output: [10, 11, 12, 13, 14]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WtWNjK8nA_2b","executionInfo":{"status":"ok","timestamp":1603633978289,"user_tz":-360,"elapsed":1214,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"15fd0e0e-f6d1-4241-9e1b-815f5095ed42","colab":{"base_uri":"https://localhost:8080/","height":176}},"source":["# import project_tests as tests\n","def pad(x, length=None):\n","    if length is None:\n","        length = max([len(sentence) for sentence in x])\n","    return pad_sequences(x, maxlen = length, padding = 'post')\n","test_pad(pad)\n","# Pad Tokenized output\n","test_pad = pad(text_tokenized)\n","for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n","    print('Sequence {} in x'.format(sample_i + 1))\n","    print('  Input:  {}'.format(np.array(token_sent)))\n","    print('  Output: {}'.format(pad_sent))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Sequence 1 in x\n","  Input:  [1 2 3 4 5]\n","  Output: [1 2 3 4 5]\n","Sequence 2 in x\n","  Input:  [6 7 8 9]\n","  Output: [6 7 8 9 0]\n","Sequence 3 in x\n","  Input:  [10 11 12 13 14]\n","  Output: [10 11 12 13 14]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uHgYVSRXA_78","executionInfo":{"status":"ok","timestamp":1603633986437,"user_tz":-360,"elapsed":4950,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"b1f7d809-66b6-4714-d3bd-80fc4036aaec","colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["def preprocess(x, y):\n","    preprocess_x, x_tk = tokenize(x)\n","    preprocess_y, y_tk = tokenize(y)\n","    preprocess_x = pad(preprocess_x)\n","    preprocess_y = pad(preprocess_y)\n","    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n","    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n","    return preprocess_x, preprocess_y, x_tk, y_tk\n","\n","preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer = preprocess(english_sentences, french_sentences)    \n","max_english_sequence_length = preproc_english_sentences.shape[1]\n","max_french_sequence_length = preproc_french_sentences.shape[1]\n","english_vocab_size = len(english_tokenizer.word_index)\n","french_vocab_size = len(french_tokenizer.word_index)\n","print('Data Preprocessed')\n","print(\"Max English sentence length:\", max_english_sequence_length)\n","print(\"Max French sentence length:\", max_french_sequence_length)\n","print(\"English vocabulary size:\", english_vocab_size)\n","print(\"French vocabulary size:\", french_vocab_size)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Data Preprocessed\n","Max English sentence length: 17\n","Max French sentence length: 17\n","English vocabulary size: 12201\n","French vocabulary size: 14157\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lfa_arPoBABp","executionInfo":{"status":"ok","timestamp":1603633987555,"user_tz":-360,"elapsed":1114,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"3a8c551a-59c8-4847-9327-87daaaa37e11","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["def logits_to_text(logits, tokenizer):\n","    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n","    index_to_words[0] = '<PAD>'\n","    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n","print('`logits_to_text` function loaded.')"],"execution_count":11,"outputs":[{"output_type":"stream","text":["`logits_to_text` function loaded.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xS9e5BAoBAHZ","executionInfo":{"status":"ok","timestamp":1603657203510,"user_tz":-360,"elapsed":23213582,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"6899b696-a75f-4cb5-e186-c71d72797d13","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from keras.models import Sequential\n","def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n","    learning_rate = 1e-2\n","    rnn = GRU(128, return_sequences=True, activation=\"softmax\")\n","    \n","    embedding = Embedding(french_vocab_size, 128, input_length=input_shape[1]) \n","    logits = TimeDistributed(Dense(french_vocab_size, activation=\"softmax\"))\n","    \n","    model = Sequential()\n","    #em can only be used in first layer --> Keras Documentation\n","    model.add(embedding)\n","    model.add(rnn)\n","    model.add(logits)\n","    model.compile(loss=sparse_categorical_crossentropy,\n","                  optimizer=Adam(learning_rate),\n","                  metrics=['accuracy'])\n","    \n","    return model\n","# tests.test_embed_model(embed_model)\n","tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n","tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n","embeded_model = embed_model(\n","    tmp_x.shape,\n","    max_french_sequence_length,\n","    english_vocab_size+1,\n","    french_vocab_size+1)\n","embeded_model.fit(tmp_x, preproc_french_sentences, batch_size=100, epochs=30, validation_split=0.2)\n","print(logits_to_text(embeded_model.predict(tmp_x[:1])[0], french_tokenizer))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Epoch 1/30\n","640/640 [==============================] - 771s 1s/step - loss: 2.8284 - accuracy: 0.7509 - val_loss: 2.1117 - val_accuracy: 0.7095\n","Epoch 2/30\n","640/640 [==============================] - 767s 1s/step - loss: 1.6160 - accuracy: 0.7679 - val_loss: 2.0126 - val_accuracy: 0.7159\n","Epoch 3/30\n","640/640 [==============================] - 766s 1s/step - loss: 1.4998 - accuracy: 0.7875 - val_loss: 1.9067 - val_accuracy: 0.7373\n","Epoch 4/30\n","640/640 [==============================] - 765s 1s/step - loss: 1.3923 - accuracy: 0.7965 - val_loss: 1.8446 - val_accuracy: 0.7408\n","Epoch 5/30\n","640/640 [==============================] - 776s 1s/step - loss: 1.3162 - accuracy: 0.8003 - val_loss: 1.8044 - val_accuracy: 0.7427\n","Epoch 6/30\n","640/640 [==============================] - 775s 1s/step - loss: 1.2535 - accuracy: 0.8031 - val_loss: 1.7684 - val_accuracy: 0.7454\n","Epoch 7/30\n","640/640 [==============================] - 776s 1s/step - loss: 1.2021 - accuracy: 0.8051 - val_loss: 1.7595 - val_accuracy: 0.7460\n","Epoch 8/30\n","640/640 [==============================] - 772s 1s/step - loss: 1.1620 - accuracy: 0.8070 - val_loss: 1.7355 - val_accuracy: 0.7476\n","Epoch 9/30\n","640/640 [==============================] - 770s 1s/step - loss: 1.1267 - accuracy: 0.8086 - val_loss: 1.7278 - val_accuracy: 0.7487\n","Epoch 10/30\n","640/640 [==============================] - 773s 1s/step - loss: 1.0960 - accuracy: 0.8100 - val_loss: 1.7256 - val_accuracy: 0.7495\n","Epoch 11/30\n","640/640 [==============================] - 772s 1s/step - loss: 1.0684 - accuracy: 0.8113 - val_loss: 1.7191 - val_accuracy: 0.7495\n","Epoch 12/30\n","640/640 [==============================] - 772s 1s/step - loss: 1.0449 - accuracy: 0.8125 - val_loss: 1.7057 - val_accuracy: 0.7506\n","Epoch 13/30\n","640/640 [==============================] - 771s 1s/step - loss: 1.0239 - accuracy: 0.8140 - val_loss: 1.7112 - val_accuracy: 0.7514\n","Epoch 14/30\n","640/640 [==============================] - 780s 1s/step - loss: 1.0040 - accuracy: 0.8155 - val_loss: 1.6991 - val_accuracy: 0.7520\n","Epoch 15/30\n","640/640 [==============================] - 773s 1s/step - loss: 0.9857 - accuracy: 0.8169 - val_loss: 1.7010 - val_accuracy: 0.7529\n","Epoch 16/30\n","640/640 [==============================] - 772s 1s/step - loss: 0.9697 - accuracy: 0.8182 - val_loss: 1.7014 - val_accuracy: 0.7535\n","Epoch 17/30\n","640/640 [==============================] - 766s 1s/step - loss: 0.9537 - accuracy: 0.8194 - val_loss: 1.7026 - val_accuracy: 0.7541\n","Epoch 18/30\n","640/640 [==============================] - 777s 1s/step - loss: 0.9395 - accuracy: 0.8205 - val_loss: 1.6973 - val_accuracy: 0.7545\n","Epoch 19/30\n","640/640 [==============================] - 785s 1s/step - loss: 0.9275 - accuracy: 0.8216 - val_loss: 1.6984 - val_accuracy: 0.7552\n","Epoch 20/30\n","640/640 [==============================] - 784s 1s/step - loss: 0.9141 - accuracy: 0.8227 - val_loss: 1.6803 - val_accuracy: 0.7562\n","Epoch 21/30\n","640/640 [==============================] - 773s 1s/step - loss: 0.9026 - accuracy: 0.8238 - val_loss: 1.6810 - val_accuracy: 0.7563\n","Epoch 22/30\n","640/640 [==============================] - 769s 1s/step - loss: 0.8920 - accuracy: 0.8247 - val_loss: 1.6811 - val_accuracy: 0.7563\n","Epoch 23/30\n","640/640 [==============================] - 768s 1s/step - loss: 0.8828 - accuracy: 0.8257 - val_loss: 1.6807 - val_accuracy: 0.7570\n","Epoch 24/30\n","640/640 [==============================] - 769s 1s/step - loss: 0.8732 - accuracy: 0.8268 - val_loss: 1.6784 - val_accuracy: 0.7573\n","Epoch 25/30\n","640/640 [==============================] - 767s 1s/step - loss: 0.8651 - accuracy: 0.8277 - val_loss: 1.6808 - val_accuracy: 0.7572\n","Epoch 26/30\n","640/640 [==============================] - 766s 1s/step - loss: 0.8575 - accuracy: 0.8286 - val_loss: 1.6868 - val_accuracy: 0.7581\n","Epoch 27/30\n","640/640 [==============================] - 767s 1s/step - loss: 0.8502 - accuracy: 0.8292 - val_loss: 1.6815 - val_accuracy: 0.7582\n","Epoch 28/30\n","640/640 [==============================] - 777s 1s/step - loss: 0.8440 - accuracy: 0.8301 - val_loss: 1.6801 - val_accuracy: 0.7588\n","Epoch 29/30\n","640/640 [==============================] - 781s 1s/step - loss: 0.8369 - accuracy: 0.8310 - val_loss: 1.6815 - val_accuracy: 0.7587\n","Epoch 30/30\n","640/640 [==============================] - 774s 1s/step - loss: 0.8310 - accuracy: 0.8318 - val_loss: 1.6877 - val_accuracy: 0.7590\n","ist <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UhWA7Ql8BAOC","executionInfo":{"status":"ok","timestamp":1603658152803,"user_tz":-360,"elapsed":10516,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"ae5b75c9-5184-4893-b6a2-7f2c337a9d30","colab":{"base_uri":"https://localhost:8080/","height":123}},"source":["def final_predictions(x, y, x_tk, y_tk):\n","    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n","    y_id_to_word[0] = '<PAD>'\n","    sentence = ''\n","    sentence = [x_tk.word_index[word] for word in sentence.split()]\n","    debug1 = sentence\n","    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n","    debug2 = sentence\n","    sentences = np.array([sentence[0], x[0]])\n","    debug3 = sentences\n","    predictions = embeded_model.predict(sentences, len(sentences))\n","    debug4 = predictions\n","    print('Sample 1:')\n","    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n","    print('ওহে')\n","    print('Sample 2:')\n","    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n","    print(' '.join([y_id_to_word[np.max(x)] for x in y[1209]]))\n","    a = []\n","    for i in range(0,len(french_sentences)):\n","        debug5 = \" \".join([y_id_to_word[np.max(x)] for x in y[i]])\n","        if i  == 1:\n","            debug4 = debug5\n","        a.append(debug5)        \n","    from pandas import DataFrame\n","    df = DataFrame(a,columns=[\"predicted string\"])\n","    df[\"predicted string\"]= df[\"predicted string\"].str.replace(\"<PAD>\", \"\", case = False) \n","    df[\"actual language\"] = french_sentences\n","\n","    \n","    return debug1, debug2, debug3, debug4, debug5, a, df\n","debug1, debug2, debug3, debug4, debug5,  a, df = final_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)\n","#df.to_csv(\"jekhane khushi save kore ne sagol.csv\")"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Sample 1:\n","ich ich <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","ওহে\n","Sample 2:\n","ist <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","prüfen sie das nach <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0LWFLlgYBAVw","executionInfo":{"status":"ok","timestamp":1603658172701,"user_tz":-360,"elapsed":943,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}}},"source":["df_test = df"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"nGzYHt3xBAnz","executionInfo":{"status":"ok","timestamp":1603658174244,"user_tz":-360,"elapsed":997,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}}},"source":["remove_characters = [\"?\", \".\",\"!\",\",\"]\n","\n","for c in remove_characters:\n","    df_test[\"actual language\"] =  df_test[\"actual language\"].str.replace(c,\"\")\n","\n","df_test[\"actual language\"] = df_test[\"actual language\"] .str.lower()"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"4fNN4lPWBAu2","executionInfo":{"status":"ok","timestamp":1603658176913,"user_tz":-360,"elapsed":958,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"70c227d6-ca49-487f-e000-24c50ac5188e","colab":{"base_uri":"https://localhost:8080/","height":406}},"source":["df_test"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>predicted string</th>\n","      <th>actual language</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>geh</td>\n","      <td>geh</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>hallo</td>\n","      <td>hallo</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>grüß gott</td>\n","      <td>grüß gott</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>lauf</td>\n","      <td>lauf</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>lauf</td>\n","      <td>lauf</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>79995</th>\n","      <td>ich habe nicht so viel mut wie ihr</td>\n","      <td>ich habe nicht so viel mut wie ihr</td>\n","    </tr>\n","    <tr>\n","      <th>79996</th>\n","      <td>das habe ich nicht vor</td>\n","      <td>das habe ich nicht vor</td>\n","    </tr>\n","    <tr>\n","      <th>79997</th>\n","      <td>ich weiss toms nummer nicht</td>\n","      <td>ich weiss toms nummer nicht</td>\n","    </tr>\n","    <tr>\n","      <th>79998</th>\n","      <td>ich weiß nicht wie ich das gemacht habe</td>\n","      <td>ich weiß nicht wie ich das gemacht habe</td>\n","    </tr>\n","    <tr>\n","      <th>79999</th>\n","      <td>ich weiß nicht wie ich es machen soll</td>\n","      <td>ich weiß nicht wie ich es machen soll</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>80000 rows × 2 columns</p>\n","</div>"],"text/plain":["                                       predicted string                          actual language\n","0                                   geh                                                      geh\n","1                                 hallo                                                    hallo\n","2                              grüß gott                                               grüß gott\n","3                                  lauf                                                     lauf\n","4                                  lauf                                                     lauf\n","...                                                 ...                                      ...\n","79995       ich habe nicht so viel mut wie ihr                ich habe nicht so viel mut wie ihr\n","79996                das habe ich nicht vor                               das habe ich nicht vor\n","79997           ich weiss toms nummer nicht                          ich weiss toms nummer nicht\n","79998  ich weiß nicht wie ich das gemacht habe           ich weiß nicht wie ich das gemacht habe\n","79999    ich weiß nicht wie ich es machen soll             ich weiß nicht wie ich es machen soll\n","\n","[80000 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"oshDKGLgBA2P","executionInfo":{"status":"ok","timestamp":1603659004153,"user_tz":-360,"elapsed":823144,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"eab50976-1af6-471c-8917-c38fa004d03a","colab":{"base_uri":"https://localhost:8080/","height":158}},"source":["col_1 = df['predicted string'].tolist()\n","col_2 = df[\"actual language\"].tolist()\n","\n","\n","from nltk.translate.bleu_score import corpus_bleu\n","#reference = [['this', 'is', 'a', 'test'], ['this', 'is' 'test']]\n","#candidate = ['this', 'is', 'a', 'test']\n","score1 = corpus_bleu(col_2, col_1, weights=(1, 0, 0, 0))\n","score2 = corpus_bleu(col_2, col_1, weights=(0.5, 0.5, 0, 0))\n","score3 = corpus_bleu(col_2, col_1, weights=(0.33, 0.33, 0.33, 0))\n","score4 = corpus_bleu(col_2, col_1, weights=(0.25, 0.25, 0.25, 0.25))\n","score21 = corpus_bleu(col_2, col_1, weights=(0.5, 0.5, 0, 0))\n","score31 = corpus_bleu(col_2, col_1, weights=(0.33, 0.33, 0.33, 0))\n","score41 = corpus_bleu(col_2, col_1, weights=(0.25, 0.25, 0.25, 0.25))\n","print(score1)\n","print(score2)\n","print(score3)\n","print(score4)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"],"name":"stderr"},{"output_type":"stream","text":["0.3561822299031535\n","0.5968100450756116\n","0.7112984167321372\n","0.7725348180345087\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eL8_ie6XB45v","executionInfo":{"status":"ok","timestamp":1603659515769,"user_tz":-360,"elapsed":473951,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"5c05b902-2b6c-4ca8-de8a-54eea0db2497","colab":{"base_uri":"https://localhost:8080/","height":158}},"source":["score11 = corpus_bleu(col_2, col_1, weights=(1, 0, 0, 0))\n","score21 = corpus_bleu(col_2, col_1, weights=(0, 1, 0, 0))\n","score31 = corpus_bleu(col_2, col_1, weights=(0, 0, 1, 0))\n","score41 = corpus_bleu(col_2, col_1, weights=(0, 0, 0, 1))\n","print(score11)\n","print(score21)\n","print(score31)\n","print(score41)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"],"name":"stderr"},{"output_type":"stream","text":["0.3561822299031535\n","1.0\n","1.0\n","1.0\n"],"name":"stdout"}]}]}