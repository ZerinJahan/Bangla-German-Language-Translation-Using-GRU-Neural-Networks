{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_BD.ipynb","provenance":[],"mount_file_id":"1TPpV4XzqoRX5_IiAssXj17FB-Sn6CQ7P","authorship_tag":"ABX9TyPbsmaXtcGzHj6TWtGPH93u"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"FpAb23lkCY2W","executionInfo":{"status":"ok","timestamp":1603681083288,"user_tz":-360,"elapsed":3055,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}}},"source":["import collections\n","import numpy as np\n","import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Model\n","from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n","from keras.layers.embeddings import Embedding\n","from keras.optimizers import Adam\n","from keras.losses import sparse_categorical_crossentropy"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"iDiGcnjiCgmZ","executionInfo":{"status":"ok","timestamp":1603681083290,"user_tz":-360,"elapsed":3035,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}}},"source":["import os\n","\n","def load_data(path):\n","    \"\"\"\n","    Load dataset\n","    \"\"\"\n","    input_file = os.path.join(path)\n","    with open(input_file, \"r\", encoding ='utf=8') as f:\n","        data = f.read()\n","\n","    return data.split('\\n')\n","\n","\n","from keras.losses import sparse_categorical_crossentropy\n","from keras.models import Sequential\n","from keras.preprocessing.text import Tokenizer\n","from keras.utils import to_categorical\n","\n","\n","def _test_model(model, input_shape, output_sequence_length, french_vocab_size):\n","    if isinstance(model, Sequential):\n","        model = model.model\n","\n","    assert model.input_shape == (None, *input_shape[1:]),'Wrong input shape. Found input shape {} using parameter input_shape={}'.format(model.input_shape, input_shape)\n","\n","    assert model.output_shape == (None, output_sequence_length, french_vocab_size),'Wrong output shape. Found output shape {} using parameters output_sequence_length={} and french_vocab_size={}'.format(model.output_shape, output_sequence_length, french_vocab_size)\n","\n","    assert len(model.loss_functions) > 0,'No loss function set.  Apply the `compile` function to the model.'\n","\n","    assert sparse_categorical_crossentropy in model.loss_functions,'Not using `sparse_categorical_crossentropy` function for loss.'\n","\n","\n","def test_tokenize(tokenize):\n","    sentences = [\n","        'আমি এই উপন্যাস আগেও পড়েছি।',\n","        'টম খুব খোলামেলা মানুষ।',\n","        'তুমি কি কখনো হেলিকপ্টারে বসেছো?']\n","    tokenized_sentences, tokenizer = tokenize(sentences)\n","    assert tokenized_sentences == tokenizer.texts_to_sequences(sentences),\\\n","        'Tokenizer returned and doesn\\'t generate the same sentences as the tokenized sentences returned. '\n","\n","\n","def test_pad(pad):\n","    tokens = [\n","        [i for i in range(4)],\n","        [i for i in range(6)],\n","        [i for i in range(3)]]\n","    padded_tokens = pad(tokens)\n","    padding_id = padded_tokens[0][-1]\n","    true_padded_tokens = np.array([\n","        [i for i in range(4)] + [padding_id]*2,\n","        [i for i in range(6)],\n","        [i for i in range(3)] + [padding_id]*3])\n","    assert isinstance(padded_tokens, np.ndarray),\\\n","        'Pad returned the wrong type.  Found {} type, expected numpy array type.'\n","    assert np.all(padded_tokens == true_padded_tokens), 'Pad returned the wrong results.'\n","\n","    padded_tokens_using_length = pad(tokens, 9)\n","    assert np.all(padded_tokens_using_length == np.concatenate((true_padded_tokens, np.full((3, 3), padding_id)), axis=1)),\\\n","        'Using length argument return incorrect results'\n","\n","\n","def test_simple_model(simple_model):\n","    input_shape = (80000, 17, 1)\n","    output_sequence_length = 17\n","    english_vocab_size = 12201\n","    french_vocab_size = 14157\n","\n","    model = simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n","    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n","\n","\n","def test_embed_model(embed_model):\n","    input_shape = (200000, 17)\n","    output_sequence_length = 17\n","    english_vocab_size = 12201\n","    french_vocab_size = 14157\n","\n","    model = embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n","    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n","\n","\n","def test_encdec_model(encdec_model):\n","    input_shape = (200000, 17, 1)\n","    output_sequence_length = 17\n","    english_vocab_size = 12201\n","    french_vocab_size = 14157\n","\n","    model = encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n","    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n","\n","\n","def test_bd_model(bd_model):\n","    input_shape = (200000, 17, 1)\n","    output_sequence_length = 17\n","    english_vocab_size = 12201\n","    french_vocab_size = 14157\n","\n","    model = bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n","    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n","\n","\n","def test_model_final(model_final):\n","    input_shape = (200000, 17)\n","    output_sequence_length = 17\n","    english_vocab_size = 12201\n","    french_vocab_size = 14157\n","\n","    model = model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n","    _test_model(model, input_shape, output_sequence_length, french_vocab_size)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ts9EW3euChLS","executionInfo":{"status":"ok","timestamp":1603681085240,"user_tz":-360,"elapsed":4954,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"70355caa-9258-49cf-abc6-4ca5cc933a4a","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["english_sentences = load_data('/content/drive/My Drive/NLP_csv/80kban.txt')\n","french_sentences = load_data('/content/drive/My Drive/NLP_csv/80kger.txt')\n","print('Dataset Loaded')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Dataset Loaded\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EC93fy3pChUo","executionInfo":{"status":"ok","timestamp":1603681085242,"user_tz":-360,"elapsed":4922,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"1840ebc3-ae71-47f2-d76a-057564370bd5","colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["for sample_i in range(2):\n","    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n","    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["small_vocab_en Line 1:  যান.\n","small_vocab_fr Line 1:  Geh.\n","small_vocab_en Line 2:  নমস্কার!\n","small_vocab_fr Line 2:  Hallo!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"48UYdS-9Chgn","executionInfo":{"status":"ok","timestamp":1603681085243,"user_tz":-360,"elapsed":4891,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"731ce8c4-de5c-4275-841c-f26877d209c0","colab":{"base_uri":"https://localhost:8080/","height":176}},"source":["english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n","french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n","print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n","print('{} unique English words.'.format(len(english_words_counter)))\n","print('10 Most common words in the English dataset:')\n","print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n","print()\n","print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n","print('{} unique French words.'.format(len(french_words_counter)))\n","print('10 Most common words in the French dataset:')\n","print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["335708 English words.\n","14101 unique English words.\n","10 Most common words in the English dataset:\n","\"আমি\" \"টম\" \"এটা\" \"কি\" \"তুমি\" \"আমার\" \"না।\" \"তোমার\" \"সে\" \"একটা\"\n","\n","356660 French words.\n","23231 unique French words.\n","10 Most common words in the French dataset:\n","\"Tom\" \"Ich\" \"ist\" \"Sie\" \"nicht\" \"das\" \"du\" \"Das\" \"hat\" \"Er\"\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kTKh0BXSChrn","executionInfo":{"status":"ok","timestamp":1603681085613,"user_tz":-360,"elapsed":4675,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"95c6ed43-15c0-4adf-d714-494bcc42df06","colab":{"base_uri":"https://localhost:8080/","height":232}},"source":["def tokenize(x):\n","    x_tk = Tokenizer(char_level = False)\n","    x_tk.fit_on_texts(x)\n","    return x_tk.texts_to_sequences(x), x_tk\n","text_sentences = [\n","    'আমি এই উপন্যাস আগেও পড়েছি।',\n","    'টম খুব খোলামেলা মানুষ।',\n","    'তুমি কি কখনো হেলিকপ্টারে বসেছো?']\n","text_tokenized, text_tokenizer = tokenize(text_sentences)\n","print(text_tokenizer.word_index)\n","print()\n","for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n","    print('Sequence {} in x'.format(sample_i + 1))\n","    print('  Input:  {}'.format(sent))\n","    print('  Output: {}'.format(token_sent))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["{'আমি': 1, 'এই': 2, 'উপন্যাস': 3, 'আগেও': 4, 'পড়েছি।': 5, 'টম': 6, 'খুব': 7, 'খোলামেলা': 8, 'মানুষ।': 9, 'তুমি': 10, 'কি': 11, 'কখনো': 12, 'হেলিকপ্টারে': 13, 'বসেছো': 14}\n","\n","Sequence 1 in x\n","  Input:  আমি এই উপন্যাস আগেও পড়েছি।\n","  Output: [1, 2, 3, 4, 5]\n","Sequence 2 in x\n","  Input:  টম খুব খোলামেলা মানুষ।\n","  Output: [6, 7, 8, 9]\n","Sequence 3 in x\n","  Input:  তুমি কি কখনো হেলিকপ্টারে বসেছো?\n","  Output: [10, 11, 12, 13, 14]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wC-wp2p_Ch34","executionInfo":{"status":"ok","timestamp":1603681086625,"user_tz":-360,"elapsed":1000,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"acbc6981-b20f-4eb9-bc35-928b71969f0b","colab":{"base_uri":"https://localhost:8080/","height":176}},"source":["# import project_tests as tests\n","def pad(x, length=None):\n","    if length is None:\n","        length = max([len(sentence) for sentence in x])\n","    return pad_sequences(x, maxlen = length, padding = 'post')\n","test_pad(pad)\n","# Pad Tokenized output\n","test_pad = pad(text_tokenized)\n","for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n","    print('Sequence {} in x'.format(sample_i + 1))\n","    print('  Input:  {}'.format(np.array(token_sent)))\n","    print('  Output: {}'.format(pad_sent))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Sequence 1 in x\n","  Input:  [1 2 3 4 5]\n","  Output: [1 2 3 4 5]\n","Sequence 2 in x\n","  Input:  [6 7 8 9]\n","  Output: [6 7 8 9 0]\n","Sequence 3 in x\n","  Input:  [10 11 12 13 14]\n","  Output: [10 11 12 13 14]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"80L1CxXrCiF4","executionInfo":{"status":"ok","timestamp":1603681090973,"user_tz":-360,"elapsed":4918,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"5116942f-a4e8-4993-d38b-e7e08034fa96","colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["def preprocess(x, y):\n","    preprocess_x, x_tk = tokenize(x)\n","    preprocess_y, y_tk = tokenize(y)\n","    preprocess_x = pad(preprocess_x)\n","    preprocess_y = pad(preprocess_y)\n","    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n","    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n","    return preprocess_x, preprocess_y, x_tk, y_tk\n","\n","preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer = preprocess(english_sentences, french_sentences)    \n","max_english_sequence_length = preproc_english_sentences.shape[1]\n","max_french_sequence_length = preproc_french_sentences.shape[1]\n","english_vocab_size = len(english_tokenizer.word_index)\n","french_vocab_size = len(french_tokenizer.word_index)\n","print('Data Preprocessed')\n","print(\"Max English sentence length:\", max_english_sequence_length)\n","print(\"Max French sentence length:\", max_french_sequence_length)\n","print(\"English vocabulary size:\", english_vocab_size)\n","print(\"French vocabulary size:\", french_vocab_size)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Data Preprocessed\n","Max English sentence length: 17\n","Max French sentence length: 17\n","English vocabulary size: 12201\n","French vocabulary size: 14157\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Li-U_dSUCiRH","executionInfo":{"status":"ok","timestamp":1603681091967,"user_tz":-360,"elapsed":963,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"edbf2adf-1b5a-49d6-ccc7-91be1a75d03d","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["def logits_to_text(logits, tokenizer):\n","    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n","    index_to_words[0] = '<PAD>'\n","    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n","print('`logits_to_text` function loaded.')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["`logits_to_text` function loaded.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EzmPka0rCib_","executionInfo":{"status":"ok","timestamp":1603707993550,"user_tz":-360,"elapsed":26902514,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"dea84ba5-04fe-42ef-a9a9-a6118afd09e5","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n","   \n","    learning_rate = 1e-2\n","    model = Sequential()\n","    model.add(Bidirectional(GRU(128, return_sequences = True, dropout = 0.1), \n","                           input_shape = input_shape[1:]))\n","    model.add(TimeDistributed(Dense(french_vocab_size, activation = 'softmax')))\n","    model.compile(loss = sparse_categorical_crossentropy, \n","                 optimizer = Adam(learning_rate), \n","                 metrics = ['accuracy'])\n","    return model\n","# tests.test_bd_model(bd_model)\n","tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n","tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n","bidi_model = bd_model(\n","    tmp_x.shape,\n","    preproc_french_sentences.shape[1],\n","    len(english_tokenizer.word_index)+1,\n","    len(french_tokenizer.word_index)+1)\n","bidi_model.fit(tmp_x, preproc_french_sentences, batch_size=100, epochs=30, validation_split=0.2)\n","# Print prediction(s)\n","print(logits_to_text(bidi_model.predict(tmp_x[:1])[0], french_tokenizer))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Epoch 1/30\n","640/640 [==============================] - 885s 1s/step - loss: 1.7554 - accuracy: 0.7780 - val_loss: 2.0492 - val_accuracy: 0.7318\n","Epoch 2/30\n","640/640 [==============================] - 890s 1s/step - loss: 1.5060 - accuracy: 0.7924 - val_loss: 1.9612 - val_accuracy: 0.7398\n","Epoch 3/30\n","640/640 [==============================] - 891s 1s/step - loss: 1.4036 - accuracy: 0.7965 - val_loss: 1.9072 - val_accuracy: 0.7382\n","Epoch 4/30\n","640/640 [==============================] - 889s 1s/step - loss: 1.3435 - accuracy: 0.7990 - val_loss: 1.8838 - val_accuracy: 0.7403\n","Epoch 5/30\n","640/640 [==============================] - 893s 1s/step - loss: 1.3147 - accuracy: 0.8004 - val_loss: 1.8752 - val_accuracy: 0.7434\n","Epoch 6/30\n","640/640 [==============================] - 915s 1s/step - loss: 1.2818 - accuracy: 0.8020 - val_loss: 1.8760 - val_accuracy: 0.7399\n","Epoch 7/30\n","640/640 [==============================] - 912s 1s/step - loss: 1.2603 - accuracy: 0.8024 - val_loss: 1.8837 - val_accuracy: 0.7417\n","Epoch 8/30\n","640/640 [==============================] - 897s 1s/step - loss: 1.2379 - accuracy: 0.8034 - val_loss: 1.8671 - val_accuracy: 0.7424\n","Epoch 9/30\n","640/640 [==============================] - 900s 1s/step - loss: 1.2262 - accuracy: 0.8039 - val_loss: 1.8669 - val_accuracy: 0.7418\n","Epoch 10/30\n","640/640 [==============================] - 891s 1s/step - loss: 1.2232 - accuracy: 0.8040 - val_loss: 1.8625 - val_accuracy: 0.7435\n","Epoch 11/30\n","640/640 [==============================] - 899s 1s/step - loss: 1.2096 - accuracy: 0.8045 - val_loss: 1.8802 - val_accuracy: 0.7412\n","Epoch 12/30\n","640/640 [==============================] - 895s 1s/step - loss: 1.1964 - accuracy: 0.8049 - val_loss: 1.8620 - val_accuracy: 0.7448\n","Epoch 13/30\n","640/640 [==============================] - 900s 1s/step - loss: 1.1888 - accuracy: 0.8054 - val_loss: 1.8793 - val_accuracy: 0.7441\n","Epoch 14/30\n","640/640 [==============================] - 892s 1s/step - loss: 1.1728 - accuracy: 0.8061 - val_loss: 1.8751 - val_accuracy: 0.7426\n","Epoch 15/30\n","640/640 [==============================] - 894s 1s/step - loss: 1.1649 - accuracy: 0.8068 - val_loss: 1.8623 - val_accuracy: 0.7449\n","Epoch 16/30\n","640/640 [==============================] - 896s 1s/step - loss: 1.1573 - accuracy: 0.8070 - val_loss: 1.8870 - val_accuracy: 0.7411\n","Epoch 17/30\n","640/640 [==============================] - 889s 1s/step - loss: 1.1528 - accuracy: 0.8075 - val_loss: 1.8880 - val_accuracy: 0.7423\n","Epoch 18/30\n","640/640 [==============================] - 889s 1s/step - loss: 1.1538 - accuracy: 0.8072 - val_loss: 1.9072 - val_accuracy: 0.7431\n","Epoch 19/30\n","640/640 [==============================] - 893s 1s/step - loss: 1.1450 - accuracy: 0.8076 - val_loss: 1.8867 - val_accuracy: 0.7449\n","Epoch 20/30\n","640/640 [==============================] - 896s 1s/step - loss: 1.1352 - accuracy: 0.8081 - val_loss: 1.8972 - val_accuracy: 0.7440\n","Epoch 21/30\n","640/640 [==============================] - 899s 1s/step - loss: 1.1283 - accuracy: 0.8085 - val_loss: 1.9103 - val_accuracy: 0.7420\n","Epoch 22/30\n","640/640 [==============================] - 893s 1s/step - loss: 1.1293 - accuracy: 0.8083 - val_loss: 1.9231 - val_accuracy: 0.7423\n","Epoch 23/30\n","640/640 [==============================] - 900s 1s/step - loss: 1.1314 - accuracy: 0.8084 - val_loss: 1.9139 - val_accuracy: 0.7429\n","Epoch 24/30\n","640/640 [==============================] - 895s 1s/step - loss: 1.1241 - accuracy: 0.8088 - val_loss: 1.9178 - val_accuracy: 0.7413\n","Epoch 25/30\n","640/640 [==============================] - 901s 1s/step - loss: 1.1285 - accuracy: 0.8086 - val_loss: 1.9139 - val_accuracy: 0.7434\n","Epoch 26/30\n","640/640 [==============================] - 890s 1s/step - loss: 1.1205 - accuracy: 0.8091 - val_loss: 1.9201 - val_accuracy: 0.7423\n","Epoch 27/30\n","640/640 [==============================] - 890s 1s/step - loss: 1.1272 - accuracy: 0.8086 - val_loss: 1.9181 - val_accuracy: 0.7431\n","Epoch 28/30\n","640/640 [==============================] - 891s 1s/step - loss: 1.1167 - accuracy: 0.8089 - val_loss: 1.9346 - val_accuracy: 0.7411\n","Epoch 29/30\n","640/640 [==============================] - 899s 1s/step - loss: 1.1062 - accuracy: 0.8096 - val_loss: 1.9342 - val_accuracy: 0.7424\n","Epoch 30/30\n","640/640 [==============================] - 896s 1s/step - loss: 1.1091 - accuracy: 0.8093 - val_loss: 1.9244 - val_accuracy: 0.7436\n","sie ist <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MSkYSUv1Cin_","executionInfo":{"status":"error","timestamp":1603713818733,"user_tz":-360,"elapsed":1308,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"27c8d9b0-6644-4e57-bd48-a801a1a2c673","colab":{"base_uri":"https://localhost:8080/","height":363}},"source":["def final_predictions(x, y, x_tk, y_tk):\n","    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n","    y_id_to_word[0] = '<PAD>'\n","    sentence = ''\n","    sentence = [x_tk.word_index[word] for word in sentence.split()]\n","    debug1 = sentence\n","    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n","    debug2 = sentence\n","    sentences = np.array([sentence[0], x[0]])\n","    debug3 = sentences\n","    predictions = bidi_model.predict(sentences, len(sentences))\n","    debug4 = predictions\n","    print('Sample 1:')\n","    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n","    print('ওহে')\n","    print('Sample 2:')\n","    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n","    print(' '.join([y_id_to_word[np.max(x)] for x in y[1209]]))\n","    a = []\n","    for i in range(0,len(french_sentences)):\n","        debug5 = \" \".join([y_id_to_word[np.max(x)] for x in y[i]])\n","        if i  == 1:\n","            debug4 = debug5\n","        a.append(debug5)        \n","    from pandas import DataFrame\n","    df = DataFrame(a,columns=[\"predicted string\"])\n","    df[\"predicted string\"]= df[\"predicted string\"].str.replace(\"<PAD>\", \"\", case = False) \n","    df[\"actual language\"] = french_sentences\n","\n","    \n","    return debug1, debug2, debug3, debug4, debug5, a, df\n","debug1, debug2, debug3, debug4, debug5,  a, df = final_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)\n","#df.to_csv(\"jekhane khushi save kore ne sagol.csv\")"],"execution_count":13,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-964660e34f86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdebug1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mdebug1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug5\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreproc_english_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreproc_french_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menglish_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrench_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;31m#df.to_csv(\"jekhane khushi save kore ne sagol.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-964660e34f86>\u001b[0m in \u001b[0;36mfinal_predictions\u001b[0;34m(x, y, x_tk, y_tk)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdebug3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mdebug4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sample 1:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}]},{"cell_type":"code","metadata":{"id":"u4yk-EEzCiy6"},"source":["df_test = df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kNMfgKt_Ci9h"},"source":["remove_characters = [\"?\", \".\",\"!\",\",\"]\n","\n","for c in remove_characters:\n","    df_test[\"actual language\"] =  df_test[\"actual language\"].str.replace(c,\"\")\n","\n","df_test[\"actual language\"] = df_test[\"actual language\"] .str.lower()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Or5CtbHLCjHx"},"source":["df_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iplu3dplCjU3"},"source":["col_1 = df['predicted string'].tolist()\n","col_2 = df[\"actual language\"].tolist()\n","\n","\n","from nltk.translate.bleu_score import corpus_bleu\n","#reference = [['this', 'is', 'a', 'test'], ['this', 'is' 'test']]\n","#candidate = ['this', 'is', 'a', 'test']\n","score1 = corpus_bleu(col_2, col_1, weights=(1, 0, 0, 0))\n","score2 = corpus_bleu(col_2, col_1, weights=(0.5, 0.5, 0, 0))\n","score3 = corpus_bleu(col_2, col_1, weights=(0.33, 0.33, 0.33, 0))\n","score4 = corpus_bleu(col_2, col_1, weights=(0.25, 0.25, 0.25, 0.25))\n","score21 = corpus_bleu(col_2, col_1, weights=(0.5, 0.5, 0, 0))\n","score31 = corpus_bleu(col_2, col_1, weights=(0.33, 0.33, 0.33, 0))\n","score41 = corpus_bleu(col_2, col_1, weights=(0.25, 0.25, 0.25, 0.25))\n","print(score1)\n","print(score2)\n","print(score3)\n","print(score4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-tENe6TeCjfa"},"source":["score11 = corpus_bleu(col_2, col_1, weights=(1, 0, 0, 0))\n","score21 = corpus_bleu(col_2, col_1, weights=(0, 1, 0, 0))\n","score31 = corpus_bleu(col_2, col_1, weights=(0, 0, 1, 0))\n","score41 = corpus_bleu(col_2, col_1, weights=(0, 0, 0, 1))\n","print(score11)\n","print(score21)\n","print(score31)\n","print(score41)"],"execution_count":null,"outputs":[]}]}