{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_Hybrid.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1rklswiUCQZwUU8J_DI60RBW2KvvkNipY","authorship_tag":"ABX9TyNPsVqgg2ESUUb30LxS3uMq"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"5oCtRsW6FFQv","executionInfo":{"status":"ok","timestamp":1603720058691,"user_tz":-360,"elapsed":2566,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}}},"source":["import collections\n","import numpy as np\n","import pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Model\n","from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n","from keras.layers.embeddings import Embedding\n","from keras.optimizers import Adam\n","from keras.losses import sparse_categorical_crossentropy"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"PNWnWe89FQ0O","executionInfo":{"status":"ok","timestamp":1603720063243,"user_tz":-360,"elapsed":1569,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}}},"source":["import os\n","\n","def load_data(path):\n","    \"\"\"\n","    Load dataset\n","    \"\"\"\n","    input_file = os.path.join(path)\n","    with open(input_file, \"r\", encoding ='utf=8') as f:\n","        data = f.read()\n","\n","    return data.split('\\n')\n","\n","\n","from keras.losses import sparse_categorical_crossentropy\n","from keras.models import Sequential\n","from keras.preprocessing.text import Tokenizer\n","from keras.utils import to_categorical\n","\n","\n","def _test_model(model, input_shape, output_sequence_length, french_vocab_size):\n","    if isinstance(model, Sequential):\n","        model = model.model\n","\n","    assert model.input_shape == (None, *input_shape[1:]),'Wrong input shape. Found input shape {} using parameter input_shape={}'.format(model.input_shape, input_shape)\n","\n","    assert model.output_shape == (None, output_sequence_length, french_vocab_size),'Wrong output shape. Found output shape {} using parameters output_sequence_length={} and french_vocab_size={}'.format(model.output_shape, output_sequence_length, french_vocab_size)\n","\n","    assert len(model.loss_functions) > 0,'No loss function set.  Apply the `compile` function to the model.'\n","\n","    assert sparse_categorical_crossentropy in model.loss_functions,'Not using `sparse_categorical_crossentropy` function for loss.'\n","\n","\n","def test_tokenize(tokenize):\n","    sentences = [\n","        'আমি এই উপন্যাস আগেও পড়েছি।',\n","        'টম খুব খোলামেলা মানুষ।',\n","        'তুমি কি কখনো হেলিকপ্টারে বসেছো?']\n","    tokenized_sentences, tokenizer = tokenize(sentences)\n","    assert tokenized_sentences == tokenizer.texts_to_sequences(sentences),\\\n","        'Tokenizer returned and doesn\\'t generate the same sentences as the tokenized sentences returned. '\n","\n","\n","def test_pad(pad):\n","    tokens = [\n","        [i for i in range(4)],\n","        [i for i in range(6)],\n","        [i for i in range(3)]]\n","    padded_tokens = pad(tokens)\n","    padding_id = padded_tokens[0][-1]\n","    true_padded_tokens = np.array([\n","        [i for i in range(4)] + [padding_id]*2,\n","        [i for i in range(6)],\n","        [i for i in range(3)] + [padding_id]*3])\n","    assert isinstance(padded_tokens, np.ndarray),\\\n","        'Pad returned the wrong type.  Found {} type, expected numpy array type.'\n","    assert np.all(padded_tokens == true_padded_tokens), 'Pad returned the wrong results.'\n","\n","    padded_tokens_using_length = pad(tokens, 9)\n","    assert np.all(padded_tokens_using_length == np.concatenate((true_padded_tokens, np.full((3, 3), padding_id)), axis=1)),\\\n","        'Using length argument return incorrect results'\n","\n","\n","def test_simple_model(simple_model):\n","    input_shape = (80000, 17, 1)\n","    output_sequence_length = 17\n","    english_vocab_size = 12201\n","    french_vocab_size = 14157\n","\n","    model = simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n","    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n","\n","\n","def test_embed_model(embed_model):\n","    input_shape = (200000, 17)\n","    output_sequence_length = 17\n","    english_vocab_size = 12201\n","    french_vocab_size = 14157\n","\n","    model = embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n","    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n","\n","\n","def test_encdec_model(encdec_model):\n","    input_shape = (200000, 17, 1)\n","    output_sequence_length = 17\n","    english_vocab_size = 12201\n","    french_vocab_size = 14157\n","\n","    model = encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n","    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n","\n","\n","def test_bd_model(bd_model):\n","    input_shape = (200000, 17, 1)\n","    output_sequence_length = 17\n","    english_vocab_size = 12201\n","    french_vocab_size = 14157\n","\n","    model = bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n","    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n","\n","\n","def test_model_final(model_final):\n","    input_shape = (200000, 17)\n","    output_sequence_length = 17\n","    english_vocab_size = 12201\n","    french_vocab_size = 14157\n","\n","    model = model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n","    _test_model(model, input_shape, output_sequence_length, french_vocab_size)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"8rjUQiJ9FQ-1","executionInfo":{"status":"ok","timestamp":1603720071642,"user_tz":-360,"elapsed":5945,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"bdb0b992-35d0-4487-da92-31fb41fb011b","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["english_sentences = load_data('/content/drive/My Drive/NLP_csv/80kban.txt')\n","french_sentences = load_data('/content/drive/My Drive/NLP_csv/80kger.txt')\n","print('Dataset Loaded')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Dataset Loaded\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Z6KD9oQbFRK2","executionInfo":{"status":"ok","timestamp":1603720071646,"user_tz":-360,"elapsed":4654,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"5dbb42da-0431-4f04-8dbd-33cd89b8e553","colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["for sample_i in range(2):\n","    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n","    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["small_vocab_en Line 1:  যান.\n","small_vocab_fr Line 1:  Geh.\n","small_vocab_en Line 2:  নমস্কার!\n","small_vocab_fr Line 2:  Hallo!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0pgs5vvCFRVd","executionInfo":{"status":"ok","timestamp":1603720071648,"user_tz":-360,"elapsed":2191,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"08809045-3610-4025-8d36-63c600f11ff2","colab":{"base_uri":"https://localhost:8080/","height":176}},"source":["english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n","french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n","print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n","print('{} unique English words.'.format(len(english_words_counter)))\n","print('10 Most common words in the English dataset:')\n","print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n","print()\n","print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n","print('{} unique French words.'.format(len(french_words_counter)))\n","print('10 Most common words in the French dataset:')\n","print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["335708 English words.\n","14101 unique English words.\n","10 Most common words in the English dataset:\n","\"আমি\" \"টম\" \"এটা\" \"কি\" \"তুমি\" \"আমার\" \"না।\" \"তোমার\" \"সে\" \"একটা\"\n","\n","356660 French words.\n","23231 unique French words.\n","10 Most common words in the French dataset:\n","\"Tom\" \"Ich\" \"ist\" \"Sie\" \"nicht\" \"das\" \"du\" \"Das\" \"hat\" \"Er\"\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U4VWYV7jFRf3","executionInfo":{"status":"ok","timestamp":1603720075104,"user_tz":-360,"elapsed":1420,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"b96b4d40-7dc2-4136-abc2-a3445230b395","colab":{"base_uri":"https://localhost:8080/","height":232}},"source":["def tokenize(x):\n","    x_tk = Tokenizer(char_level = False)\n","    x_tk.fit_on_texts(x)\n","    return x_tk.texts_to_sequences(x), x_tk\n","text_sentences = [\n","    'আমি এই উপন্যাস আগেও পড়েছি।',\n","    'টম খুব খোলামেলা মানুষ।',\n","    'তুমি কি কখনো হেলিকপ্টারে বসেছো?']\n","text_tokenized, text_tokenizer = tokenize(text_sentences)\n","print(text_tokenizer.word_index)\n","print()\n","for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n","    print('Sequence {} in x'.format(sample_i + 1))\n","    print('  Input:  {}'.format(sent))\n","    print('  Output: {}'.format(token_sent))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["{'আমি': 1, 'এই': 2, 'উপন্যাস': 3, 'আগেও': 4, 'পড়েছি।': 5, 'টম': 6, 'খুব': 7, 'খোলামেলা': 8, 'মানুষ।': 9, 'তুমি': 10, 'কি': 11, 'কখনো': 12, 'হেলিকপ্টারে': 13, 'বসেছো': 14}\n","\n","Sequence 1 in x\n","  Input:  আমি এই উপন্যাস আগেও পড়েছি।\n","  Output: [1, 2, 3, 4, 5]\n","Sequence 2 in x\n","  Input:  টম খুব খোলামেলা মানুষ।\n","  Output: [6, 7, 8, 9]\n","Sequence 3 in x\n","  Input:  তুমি কি কখনো হেলিকপ্টারে বসেছো?\n","  Output: [10, 11, 12, 13, 14]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"p6X1vtoXFRrF","executionInfo":{"status":"ok","timestamp":1603720078484,"user_tz":-360,"elapsed":1540,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"38295079-ccec-48a3-9971-87400292c2ac","colab":{"base_uri":"https://localhost:8080/","height":176}},"source":["# import project_tests as tests\n","def pad(x, length=None):\n","    if length is None:\n","        length = max([len(sentence) for sentence in x])\n","    return pad_sequences(x, maxlen = length, padding = 'post')\n","test_pad(pad)\n","# Pad Tokenized output\n","test_pad = pad(text_tokenized)\n","for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n","    print('Sequence {} in x'.format(sample_i + 1))\n","    print('  Input:  {}'.format(np.array(token_sent)))\n","    print('  Output: {}'.format(pad_sent))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Sequence 1 in x\n","  Input:  [1 2 3 4 5]\n","  Output: [1 2 3 4 5]\n","Sequence 2 in x\n","  Input:  [6 7 8 9]\n","  Output: [6 7 8 9 0]\n","Sequence 3 in x\n","  Input:  [10 11 12 13 14]\n","  Output: [10 11 12 13 14]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GboDNE2RFR11","executionInfo":{"status":"ok","timestamp":1603720085386,"user_tz":-360,"elapsed":4492,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"a54547de-fad5-42e7-dc5d-e6b14ac67d83","colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["def preprocess(x, y):\n","    preprocess_x, x_tk = tokenize(x)\n","    preprocess_y, y_tk = tokenize(y)\n","    preprocess_x = pad(preprocess_x)\n","    preprocess_y = pad(preprocess_y)\n","    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n","    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n","    return preprocess_x, preprocess_y, x_tk, y_tk\n","\n","preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer = preprocess(english_sentences, french_sentences)    \n","max_english_sequence_length = preproc_english_sentences.shape[1]\n","max_french_sequence_length = preproc_french_sentences.shape[1]\n","english_vocab_size = len(english_tokenizer.word_index)\n","french_vocab_size = len(french_tokenizer.word_index)\n","print('Data Preprocessed')\n","print(\"Max English sentence length:\", max_english_sequence_length)\n","print(\"Max French sentence length:\", max_french_sequence_length)\n","print(\"English vocabulary size:\", english_vocab_size)\n","print(\"French vocabulary size:\", french_vocab_size)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Data Preprocessed\n","Max English sentence length: 17\n","Max French sentence length: 17\n","English vocabulary size: 12201\n","French vocabulary size: 14157\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kyT9aW-iFSAo","executionInfo":{"status":"ok","timestamp":1603720086698,"user_tz":-360,"elapsed":1295,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"2875a368-793d-42c6-9536-c5b51249e4a0","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["def logits_to_text(logits, tokenizer):\n","    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n","    index_to_words[0] = '<PAD>'\n","    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n","print('`logits_to_text` function loaded.')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["`logits_to_text` function loaded.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VuRUp5X1FSLf","executionInfo":{"status":"ok","timestamp":1603720092346,"user_tz":-360,"elapsed":1088,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"a0859e9e-024f-4149-f7be-e41b90c2d2c3","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n","  \n","    model = Sequential()\n","    model.add(Embedding(input_dim=english_vocab_size,output_dim=128,input_length=input_shape[1]))\n","    model.add(Bidirectional(GRU(128,return_sequences=False)))\n","    model.add(RepeatVector(output_sequence_length))\n","    model.add(Bidirectional(GRU(128,return_sequences=True)))\n","    model.add(TimeDistributed(Dense(french_vocab_size,activation='softmax')))\n","    learning_rate = 1e-2\n","    \n","    model.compile(loss = sparse_categorical_crossentropy, \n","                 optimizer = Adam(learning_rate), \n","                 metrics = ['accuracy'])\n","    \n","    return model\n","# tests.test_model_final(model_final)\n","print('Final Model Loaded')"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Final Model Loaded\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ptIxgqFfFSV_","outputId":"56b72fba-c249-4c62-e422-3ea4738de7bd","colab":{"base_uri":"https://localhost:8080/","height":905}},"source":["tmp_X = pad(preproc_english_sentences)\n","model = model_final(tmp_X.shape,preproc_french_sentences.shape[1],len(english_tokenizer.word_index)+1,len(french_tokenizer.word_index)+1)\n","model.fit(tmp_X, preproc_french_sentences, batch_size = 100, epochs = 30, validation_split = 0.2)\n","model.save('Finalmodel.model')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/30\n","640/640 [==============================] - 1097s 2s/step - loss: 2.0160 - accuracy: 0.7764 - val_loss: 2.4143 - val_accuracy: 0.7405\n","Epoch 2/30\n","640/640 [==============================] - 1088s 2s/step - loss: 1.7830 - accuracy: 0.8084 - val_loss: 2.3059 - val_accuracy: 0.7562\n","Epoch 3/30\n","640/640 [==============================] - 1084s 2s/step - loss: 1.6884 - accuracy: 0.8206 - val_loss: 2.2380 - val_accuracy: 0.7622\n","Epoch 4/30\n","640/640 [==============================] - 1087s 2s/step - loss: 1.6349 - accuracy: 0.8283 - val_loss: 2.2626 - val_accuracy: 0.7665\n","Epoch 5/30\n","640/640 [==============================] - 1077s 2s/step - loss: 1.6070 - accuracy: 0.8322 - val_loss: 2.2032 - val_accuracy: 0.7676\n","Epoch 6/30\n","640/640 [==============================] - 1079s 2s/step - loss: 1.5864 - accuracy: 0.8349 - val_loss: 2.2154 - val_accuracy: 0.7708\n","Epoch 7/30\n","640/640 [==============================] - 1087s 2s/step - loss: 1.5712 - accuracy: 0.8366 - val_loss: 2.1919 - val_accuracy: 0.7723\n","Epoch 8/30\n","640/640 [==============================] - 1085s 2s/step - loss: 1.5482 - accuracy: 0.8394 - val_loss: 2.1793 - val_accuracy: 0.7722\n","Epoch 9/30\n","640/640 [==============================] - 1067s 2s/step - loss: 1.5136 - accuracy: 0.8414 - val_loss: 2.1217 - val_accuracy: 0.7743\n","Epoch 10/30\n","640/640 [==============================] - 1063s 2s/step - loss: 1.4787 - accuracy: 0.8422 - val_loss: 2.0756 - val_accuracy: 0.7754\n","Epoch 11/30\n","640/640 [==============================] - 1079s 2s/step - loss: 1.4368 - accuracy: 0.8443 - val_loss: 2.0126 - val_accuracy: 0.7759\n","Epoch 12/30\n","640/640 [==============================] - 1095s 2s/step - loss: 1.3482 - accuracy: 0.8452 - val_loss: 1.9456 - val_accuracy: 0.7761\n","Epoch 13/30\n","640/640 [==============================] - 1073s 2s/step - loss: 1.2926 - accuracy: 0.8463 - val_loss: 1.9223 - val_accuracy: 0.7778\n","Epoch 14/30\n","640/640 [==============================] - 1094s 2s/step - loss: 1.2303 - accuracy: 0.8470 - val_loss: 1.8716 - val_accuracy: 0.7764\n","Epoch 15/30\n","640/640 [==============================] - 1100s 2s/step - loss: 1.1734 - accuracy: 0.8477 - val_loss: 1.8272 - val_accuracy: 0.7784\n","Epoch 16/30\n","640/640 [==============================] - 1083s 2s/step - loss: 1.1243 - accuracy: 0.8485 - val_loss: 1.7945 - val_accuracy: 0.7776\n","Epoch 17/30\n","640/640 [==============================] - 1090s 2s/step - loss: 1.0713 - accuracy: 0.8494 - val_loss: 1.7518 - val_accuracy: 0.7794\n","Epoch 18/30\n","640/640 [==============================] - 1071s 2s/step - loss: 1.0261 - accuracy: 0.8503 - val_loss: 1.7533 - val_accuracy: 0.7784\n","Epoch 19/30\n","640/640 [==============================] - 1066s 2s/step - loss: 0.9941 - accuracy: 0.8512 - val_loss: 1.7146 - val_accuracy: 0.7788\n","Epoch 20/30\n","640/640 [==============================] - 1060s 2s/step - loss: 0.9689 - accuracy: 0.8517 - val_loss: 1.7051 - val_accuracy: 0.7802\n","Epoch 21/30\n","640/640 [==============================] - 1049s 2s/step - loss: 0.9480 - accuracy: 0.8522 - val_loss: 1.6822 - val_accuracy: 0.7796\n","Epoch 22/30\n","640/640 [==============================] - 1041s 2s/step - loss: 0.9292 - accuracy: 0.8531 - val_loss: 1.6748 - val_accuracy: 0.7770\n","Epoch 23/30\n","640/640 [==============================] - 1057s 2s/step - loss: 0.9091 - accuracy: 0.8534 - val_loss: 1.6769 - val_accuracy: 0.7784\n","Epoch 24/30\n","640/640 [==============================] - 1070s 2s/step - loss: 0.8965 - accuracy: 0.8540 - val_loss: 1.6555 - val_accuracy: 0.7804\n","Epoch 25/30\n"," 70/640 [==>...........................] - ETA: 14:29 - loss: 0.8550 - accuracy: 0.8585"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QnQ00GkNFSgw","executionInfo":{"status":"ok","timestamp":1603683773656,"user_tz":-360,"elapsed":11263,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"5fe24e52-fb16-4e13-d1da-c360eb840f6c","colab":{"base_uri":"https://localhost:8080/","height":123}},"source":["def final_predictions(x, y, x_tk, y_tk):\n","    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n","    y_id_to_word[0] = '<PAD>'\n","    sentence = ''\n","    sentence = [x_tk.word_index[word] for word in sentence.split()]\n","    debug1 = sentence\n","    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n","    debug2 = sentence\n","    sentences = np.array([sentence[0], x[0]])\n","    debug3 = sentences\n","    predictions = model.predict(sentences, len(sentences))\n","    debug4 = predictions\n","    print('Sample 1:')\n","    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n","    print('ওহে')\n","    print('Sample 2:')\n","    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n","    print(' '.join([y_id_to_word[np.max(x)] for x in y[1209]]))\n","    a = []\n","    for i in range(0,len(french_sentences)):\n","        debug5 = \" \".join([y_id_to_word[np.max(x)] for x in y[i]])\n","        if i  == 1:\n","            debug4 = debug5\n","        a.append(debug5)        \n","    from pandas import DataFrame\n","    df = DataFrame(a,columns=[\"predicted string\"])\n","    df[\"predicted string\"]= df[\"predicted string\"].str.replace(\"<PAD>\", \"\", case = False) \n","    df[\"actual language\"] = french_sentences\n","\n","    \n","    return debug1, debug2, debug3, debug4, debug5, a, df\n","debug1, debug2, debug3, debug4, debug5,  a, df = final_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)\n","#df.to_csv(\"jekhane khushi save kore ne sagol.csv\")"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Sample 1:\n","die andermal <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","ওহে\n","Sample 2:\n","geh sie <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","prüfen sie das nach <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w2AEjjPWFSrn","executionInfo":{"status":"ok","timestamp":1603683781630,"user_tz":-360,"elapsed":1114,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}}},"source":["df_test = df"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"fOjNctw9FS6k","executionInfo":{"status":"ok","timestamp":1603683783446,"user_tz":-360,"elapsed":1211,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}}},"source":["remove_characters = [\"?\", \".\",\"!\",\",\"]\n","\n","for c in remove_characters:\n","    df_test[\"actual language\"] =  df_test[\"actual language\"].str.replace(c,\"\")\n","\n","df_test[\"actual language\"] = df_test[\"actual language\"] .str.lower()"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"fwHckza1FTGo","executionInfo":{"status":"error","timestamp":1603720036377,"user_tz":-360,"elapsed":1373,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"f9981b7e-2258-47b4-9e0f-880262b09369","colab":{"base_uri":"https://localhost:8080/","height":167}},"source":["df_test"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-5418cae61a6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'df_test' is not defined"]}]},{"cell_type":"code","metadata":{"id":"qEANAFYEFTS2","executionInfo":{"status":"ok","timestamp":1603684613651,"user_tz":-360,"elapsed":825455,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"4fbd0910-8a2b-47d1-abd3-86b2d16ef636","colab":{"base_uri":"https://localhost:8080/","height":158}},"source":["col_1 = df['predicted string'].tolist()\n","col_2 = df[\"actual language\"].tolist()\n","\n","\n","from nltk.translate.bleu_score import corpus_bleu\n","#reference = [['this', 'is', 'a', 'test'], ['this', 'is' 'test']]\n","#candidate = ['this', 'is', 'a', 'test']\n","score1 = corpus_bleu(col_2, col_1, weights=(1, 0, 0, 0))\n","score2 = corpus_bleu(col_2, col_1, weights=(0.5, 0.5, 0, 0))\n","score3 = corpus_bleu(col_2, col_1, weights=(0.33, 0.33, 0.33, 0))\n","score4 = corpus_bleu(col_2, col_1, weights=(0.25, 0.25, 0.25, 0.25))\n","score21 = corpus_bleu(col_2, col_1, weights=(0.5, 0.5, 0, 0))\n","score31 = corpus_bleu(col_2, col_1, weights=(0.33, 0.33, 0.33, 0))\n","score41 = corpus_bleu(col_2, col_1, weights=(0.25, 0.25, 0.25, 0.25))\n","print(score1)\n","print(score2)\n","print(score3)\n","print(score4)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"],"name":"stderr"},{"output_type":"stream","text":["0.3561822299031535\n","0.5968100450756116\n","0.7112984167321372\n","0.7725348180345087\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wG_gR6MvF_Vt","executionInfo":{"status":"ok","timestamp":1603685075052,"user_tz":-360,"elapsed":1284970,"user":{"displayName":"JOY PAUL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtAXIR8xWrSQkgqla_ZQhAmp7-3aHEUv5wEncTbw=s64","userId":"08034521466942635406"}},"outputId":"d7f6856b-25a0-468f-ebbb-906e386a9446","colab":{"base_uri":"https://localhost:8080/","height":158}},"source":["score11 = corpus_bleu(col_2, col_1, weights=(1, 0, 0, 0))\n","score21 = corpus_bleu(col_2, col_1, weights=(0, 1, 0, 0))\n","score31 = corpus_bleu(col_2, col_1, weights=(0, 0, 1, 0))\n","score41 = corpus_bleu(col_2, col_1, weights=(0, 0, 0, 1))\n","print(score11)\n","print(score21)\n","print(score31)\n","print(score41)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"],"name":"stderr"},{"output_type":"stream","text":["0.3561822299031535\n","1.0\n","1.0\n","1.0\n"],"name":"stdout"}]}]}